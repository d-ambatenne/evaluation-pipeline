# Running the Evaluation Pipeline

This guide explains how to prepare any Kotlin/Gradle project for evaluation and how to run the pipeline against it.

## How the Pipeline Works

The pipeline evaluates how well LLMs can complete coding tasks in your project. For each task:

1. The model receives a task description (`TASK.md`), project structure, build config, and source files
2. It generates code (new or modified files)
3. The pipeline compiles the code and runs tests
4. If compilation or tests fail, the model gets the errors and tries again (up to `maxAttempts`)
5. Results are scored and written to reports

Each task lives on its own git branch. The model works from the `main` branch state and only sees the `TASK.md` from the task branch — the reference solution on that branch is used for LOC comparison, not shown to the model.

## Preparing Your Project

### Prerequisites

- A Kotlin/Gradle project hosted on GitHub (or available as a local clone)
- The project must compile and have tests runnable via Gradle tasks
- Git branches set up with tasks (see below)

### Step 1: Create Task Branches

Each task is a separate git branch that contains:

1. A `TASK.md` file at the repository root describing what the model should implement
2. The reference solution (the actual implementation) — used only for LOC delta comparison

**Branch workflow:**

```
main (clean starting point — the model works from here)
├── branch01 (TASK.md + reference solution for task 1)
├── branch02 (TASK.md + reference solution for task 2)
└── ...
```

The model never sees the reference solution. It only receives:
- The `TASK.md` content
- The project state on `main`
- Source files matching `contextFiles` globs (from `main`)

### Step 2: Write TASK.md Files

Each `TASK.md` should clearly describe what code to write. Good task descriptions include:

- What files to create or modify (with exact paths)
- Function signatures, data class definitions, or API contracts
- Expected behavior and edge cases
- Which existing code to integrate with

**Example:**

```markdown
# Task: Implement CRUD Routes

Create `backend/src/main/kotlin/CustomerRoutes.kt` with a function
`fun Routing.configureCustomerRoutes()` that sets up these endpoints:

1. **GET /customers** — Return all customers as JSON
2. **POST /customers** — Create a customer, return 201
3. **GET /customers/{id}** — Return customer by ID or 404
4. **DELETE /customers/{id}** — Delete customer, return 204 or 404

Wire the routes into `Application.kt` by calling `configureCustomerRoutes()`
inside the `routing { }` block.
```

**Tips for better results:**
- Include exact import paths for non-obvious APIs (especially EAP/beta libraries)
- Mention which dependencies are available in `build.gradle.kts`
- Specify the expected file paths relative to the project root

### Step 3: Ensure Tests Exist on Main

The pipeline verifies the model's code by running your project's test suite. Tests should:

- Already exist on `main` (or be generated by the model as part of the task)
- Be runnable via a single Gradle task (e.g., `:backend:test`, `:app:test`)
- Cover the functionality described in `TASK.md`

### Step 4: Create `tasks.json`

Place a `tasks.json` file at the repository root on `main`:

```json
{
  "projectName": "my-project",
  "repository": "https://github.com/user/my-project",
  "mainBranch": "main",
  "tasks": [
    {
      "id": "my-first-task",
      "branch": "branch01",
      "title": "Implement feature X",
      "difficulty": "MEDIUM",
      "tags": ["routing", "serialization"],
      "verification": {
        "compileTask": ":app:compileKotlin",
        "testTask": ":app:test"
      },
      "maxAttempts": 3,
      "expectedOutcome": "SUCCESS",
      "contextFiles": [
        "app/src/main/kotlin/**/*.kt",
        "app/src/test/kotlin/**/*.kt",
        "app/build.gradle.kts",
        "build.gradle.kts",
        "settings.gradle.kts",
        "gradle/libs.versions.toml"
      ]
    }
  ]
}
```

### Task Definition Fields

| Field | Required | Description |
|-------|----------|-------------|
| `id` | Yes | Unique task identifier (used in reports) |
| `branch` | Yes | Git branch containing `TASK.md` and reference solution |
| `title` | Yes | Human-readable task title |
| `difficulty` | Yes | `TRIVIAL`, `EASY`, `MEDIUM`, `HARD`, or `EXPERT` |
| `tags` | No | Labels for categorization (default: `[]`) |
| `verification.compileTask` | Yes | Gradle task to compile (e.g., `:app:compileKotlin`) |
| `verification.testTask` | Yes | Gradle task to run tests (e.g., `:app:test`) |
| `maxAttempts` | No | Max retry attempts per task (default: `3`) |
| `expectedOutcome` | No | `SUCCESS` (tests must pass), `PARTIAL` (compile is enough), or `FAILURE` (default: `SUCCESS`) |
| `contextFiles` | No | Glob patterns for source files to include in the prompt (default: `[]`) |

### Context Files

The `contextFiles` globs determine what source code the model sees. These are resolved against the `main` branch. Include:

- Source files the model needs to understand or modify
- Build configuration (`build.gradle.kts`, `libs.versions.toml`)
- Test files (so the model knows what's being tested)
- Config files (`application.yaml`, etc.)

**Important:** The more context you provide, the better the model performs — but there's a token budget. Files are truncated if they exceed the limit. Prioritize the most relevant files.

## Running the Pipeline

### 1. Clone Both Repositories

```bash
# Clone the evaluation pipeline
git clone https://github.com/d-ambatenne/evaluation-pipeline.git
cd evaluation-pipeline

# Clone your target project (if not already local)
git clone https://github.com/user/my-project.git /path/to/my-project
```

### 2. Set API Keys

Set environment variables for the model providers you want to use. At least one is required.

```bash
# Claude (Anthropic)
export ANTHROPIC_API_KEY="sk-ant-..."
# Or with a proxy:
export ANTHROPIC_BASE_URL="https://your-proxy.example.com"
export ANTHROPIC_AUTH_TOKEN="your-token"

# OpenAI
export OPENAI_API_KEY="sk-..."
# Or with a proxy:
export OPENAI_BASE_URL="https://your-proxy.example.com"
export OPENAI_AUTH_TOKEN="your-token"

# Gemini
export GEMINI_API_KEY="AI..."
```

### 3. Validate Setup (Dry Run)

```bash
./gradlew run --args="--repo /path/to/my-project --dry-run"
```

This checks that `tasks.json` exists, all task branches have `TASK.md` files, and API keys are configured — without making any API calls.

### 4. Run the Evaluation

```bash
# Run all tasks with all available models
./gradlew run --args="--repo /path/to/my-project --output ./results"

# Run specific tasks
./gradlew run --args="--repo /path/to/my-project --tasks my-first-task,my-second-task --output ./results"

# Run with specific models
./gradlew run --args="--repo /path/to/my-project --models claude-opus-4-6,claude-sonnet-4-5-20250929 --output ./results"

# Override max attempts
./gradlew run --args="--repo /path/to/my-project --max-attempts 5 --output ./results"

# Run models in parallel (each model gets its own sandbox)
./gradlew run --args="--repo /path/to/my-project --parallel --output ./results"
```

### CLI Options

| Option | Description |
|--------|-------------|
| `--repo <path>` | **(Required)** Path to local clone of target repository |
| `--tasks <id,id,...>` | Run only specific tasks (default: all from `tasks.json`) |
| `--models <name,...>` | Run specific model IDs (default: all with API keys set) |
| `--max-attempts <n>` | Override max attempts per task |
| `--output <dir>` | Output directory for results (default: `./results`) |
| `--parallel` | Run models in parallel |
| `--dry-run` | Validate setup without calling models |

### Available Model IDs

| Provider | Model IDs |
|----------|-----------|
| Claude | `claude-opus-4-6`, `claude-sonnet-4-5-20250929`, `claude-sonnet-4-20250514` |
| OpenAI | `gpt-4o`, `o1`, etc. |
| Gemini | `gemini-2.0-flash`, etc. |

When `--models` is not specified, the pipeline creates one provider per available API key using default models.

## Output

Results are written to the output directory:

| File | Description |
|------|-------------|
| `results.json` | Full structured results (all attempts, token usage, errors) |
| `summary.md` | Human-readable report with summary table, metrics, and per-task details |
| `comparison.md` | Cross-model comparison (only generated when 2+ models are evaluated) |

### What the Reports Show

- **Summary table**: pass/fail per task per model
- **Metrics**: first-try compile rate, test pass rate, recovery rate, token usage
- **Failure distribution**: categorized failure reasons
- **Code delta**: LOC comparison between generated and reference solution
- **Per-task details**: per-attempt token counts, compiler errors, test failures

## Typical Run Times

| Scenario | Approximate Time |
|----------|-----------------|
| 2 tasks, 1 model | ~1 min |
| 9 tasks, 1 model | ~14 min |
| 9 tasks, 2 models (sequential) | ~29 min |

Times vary based on model response latency, compilation time, and number of retry attempts.

## Troubleshooting

### "tasks.json not found"
Ensure `tasks.json` is at the root of your target repository on the `main` branch (or whichever branch is configured as `mainBranch`).

### "Cannot read TASK.md from branch"
The task branch must exist and contain a `TASK.md` file at the repository root. Verify with:
```bash
git show branch-name:TASK.md
```

### "No model providers available"
Set at least one API key environment variable (see Step 2 above).

### API errors (502, timeouts)
The pipeline handles transient API errors gracefully — they count as failed attempts but don't crash the run. If a proxy is unreliable, consider increasing `maxAttempts`.

### Models use wrong APIs
If models hallucinate APIs from other libraries, improve your `TASK.md` with:
- Exact import paths for non-standard APIs
- Links to or snippets from library documentation
- Explicit instructions like "use the `dependencies { }` DSL from `io.ktor.server.plugins.di`"
